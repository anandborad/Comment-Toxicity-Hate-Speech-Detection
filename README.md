# Comment-Toxicity-Hate-Speech-Detection
### Detecting different types of toxicity like threats, obscenity, insults, and identity-based hate given a text using Toxic Comment dataset from jigsaw on Kaggle.

The modern day internet and social media helps individual voice to reach many people. It gave people the platform to share their voice. 
The adversarial effect is many people are using it for hate speech and cyberbullying. When genuine people experience hate speech or toxic content on particular platform, they stops expressing themselves and sometimes they give up on platform as well. This could be a difficult challenge for the platform growth. 
To solve the problem I build models that detect different types of toxicity like threats, obscenity, insults, and identity-based hate given a text using Toxic Comment dataset.

We’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful. The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015.

Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.

We are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:
- toxic
- severe_toxic
- obscene
- threat
- insult
- identity_hate

The tagging was done via crowdsourcing which means that the dataset was rated by different people and the tagging might not be 100% accurate too.

The source paper also contains more interesting details about the dataset creation. (link to source paper: https://arxiv.org/pdf/1610.08914.pdf (https://arxiv.org/pdf/1610.08914.pdf))
